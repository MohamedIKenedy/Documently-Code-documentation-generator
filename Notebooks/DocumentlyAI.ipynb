{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56CzC_vIBwM8"
      },
      "outputs": [],
      "source": [
        "# training_app.py\n",
        "\n",
        "# Dependencies installation\n",
        "\n",
        "# Run these commands first:\n",
        "!pip install datasets transformers evaluate accelerate -U\n",
        "!pip install huggingface_hub\n",
        "!pip install streamlit mlflow peft\n",
        "!pip install -q gradio\n",
        "!pip install bitsandbytes\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install pyngrok\n",
        "!pip install torch>=2.0.0\n",
        "!npm install localtunnel\n",
        "!pip install mlflow\n",
        "!pip install pyngrok\n",
        "!pip install mlflow[extras]\n",
        "!pip install nltk rouge-score seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIQ_Gu2qbKVU"
      },
      "outputs": [],
      "source": [
        "!pip install databricks-cli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa-dWWZQbedN"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import mlflow\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(mlflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNoQfNRngr3k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import mlflow\n",
        "import torch\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainerCallback\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model\n",
        ")\n",
        "from mlflow.tracking import MlflowClient\n",
        "from accelerate import Accelerator\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jla-EBuwhJU_"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7KbEfNHgxAg"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n",
        "\n",
        "# Set up the MLflow tracking URI\n",
        "local_registry = \"sqlite:///mlruns.db\"\n",
        "print(f\"Running local model registry={local_registry}\")\n",
        "mlflow.set_tracking_uri(local_registry)\n",
        "\n",
        "MODEL_NAME = \"Documently_Model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model and tokenizer & Dataset prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOraBp4Gg-Nt"
      },
      "outputs": [],
      "source": [
        "def setup_model_and_tokenizer(model_name=\"google/gemma-2b\"):\n",
        "    \"\"\"Setup the model and tokenizer with MLflow tracking.\"\"\"\n",
        "    with mlflow.start_run(run_name=\"MODEL_SETUP\") as run:\n",
        "        compute_dtype = torch.float16\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "\n",
        "        # Log configuration parameters\n",
        "        mlflow.log_params({\n",
        "            \"model_name\": model_name,\n",
        "            \"compute_dtype\": str(compute_dtype),\n",
        "            \"quantization\": \"4-bit\",\n",
        "            \"quant_type\": \"nf4\"\n",
        "        })\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            model_max_length=512,\n",
        "            padding_side=\"right\",\n",
        "            use_fast=True,\n",
        "        )\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=compute_dtype,\n",
        "        )\n",
        "\n",
        "        model = prepare_model_for_kbit_training(\n",
        "            model,\n",
        "            use_gradient_checkpointing=True,\n",
        "        )\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        # Log LoRA configuration\n",
        "        mlflow.log_params({\n",
        "            \"lora_r\": lora_config.r,\n",
        "            \"lora_alpha\": lora_config.lora_alpha,\n",
        "            \"lora_dropout\": lora_config.lora_dropout\n",
        "        })\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "\n",
        "        # Log model summary\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        mlflow.log_metrics({\n",
        "            \"trainable_parameters\": trainable_params,\n",
        "            \"total_parameters\": total_params,\n",
        "            \"trainable_percentage\": (trainable_params / total_params) * 100\n",
        "        })\n",
        "\n",
        "        return model, tokenizer, run.info.run_id\n",
        "\n",
        "def load_and_prepare_dataset(num_examples=2000):\n",
        "    \"\"\"Load and prepare the dataset with MLflow tracking.\"\"\"\n",
        "    with mlflow.start_run(run_name=\"DATA_PREPARATION\") as run:\n",
        "        dataset = load_dataset(\"code_search_net\", \"python\",trust_remote_code=True)\n",
        "        df = dataset[\"train\"].to_pandas()\n",
        "        df = df[df['func_documentation_string'].str.len() > 0].reset_index(drop=True)\n",
        "        df = df.head(num_examples)\n",
        "\n",
        "        prompts = []\n",
        "        completions = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            prompt = f\"Write documentation for this Python function:\\n{row['func_code_string']}\"\n",
        "            completion = f\"\\nDocumentation:\\n{row['func_documentation_string']}\\n\"\n",
        "            prompts.append(prompt)\n",
        "            completions.append(completion)\n",
        "\n",
        "        # Enhanced dataset analysis and visualization\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        from collections import Counter\n",
        "        import numpy as np\n",
        "\n",
        "        # Calculate and plot length distributions\n",
        "        prompt_lengths = [len(p) for p in prompts]\n",
        "        completion_lengths = [len(c) for c in completions]\n",
        "\n",
        "        # Create length distribution plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist([prompt_lengths, completion_lengths], label=['Prompts', 'Completions'], bins=30, alpha=0.7)\n",
        "        plt.xlabel('Length (characters)')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Distribution of Prompt and Completion Lengths')\n",
        "        plt.legend()\n",
        "        mlflow.log_figure(plt.gcf(), \"length_distribution.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Calculate and log additional dataset statistics\n",
        "        mlflow.log_metrics({\n",
        "            \"num_examples\": len(prompts),\n",
        "            \"max_length\": max(len(p) + len(c) for p, c in zip(prompts, completions)),\n",
        "            \"avg_length\": sum(len(p) + len(c) for p, c in zip(prompts, completions)) / len(prompts),\n",
        "            \"min_prompt_length\": min(prompt_lengths),\n",
        "            \"max_prompt_length\": max(prompt_lengths),\n",
        "            \"avg_prompt_length\": np.mean(prompt_lengths),\n",
        "            \"min_completion_length\": min(completion_lengths),\n",
        "            \"max_completion_length\": max(completion_lengths),\n",
        "            \"avg_completion_length\": np.mean(completion_lengths),\n",
        "            \"std_prompt_length\": np.std(prompt_lengths),\n",
        "            \"std_completion_length\": np.std(completion_lengths)\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"prompts\": prompts,\n",
        "            \"completions\": completions\n",
        "        }, run.info.run_id\n",
        "\n",
        "def prepare_training_data(dataset_info, tokenizer):\n",
        "    \"\"\"Prepare the training data by tokenizing and creating a PyTorch Dataset.\"\"\"\n",
        "    combined_texts = [p + c for p, c in zip(dataset_info[\"prompts\"], dataset_info[\"completions\"])]\n",
        "\n",
        "    tokenized_data = tokenizer(\n",
        "        combined_texts,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    class DocumentationDataset(Dataset):\n",
        "        def __init__(self, tokenized_data):\n",
        "            self.input_ids = tokenized_data['input_ids']\n",
        "            self.attention_mask = tokenized_data['attention_mask']\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.input_ids)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return {\n",
        "                'input_ids': self.input_ids[idx],\n",
        "                'attention_mask': self.attention_mask[idx],\n",
        "                'labels': self.input_ids[idx].clone()\n",
        "            }\n",
        "\n",
        "    return DocumentationDataset(tokenized_data)\n",
        "\n",
        "\n",
        "\n",
        "class MetricsCallback(TrainerCallback):\n",
        "    \"\"\"Custom callback to log additional metrics during training.\"\"\"\n",
        "    def __init__(self, tokenizer, eval_dataset, eval_steps=100, run_id=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.eval_steps = eval_steps\n",
        "        self.losses = []\n",
        "        self.bleu_scores = []\n",
        "        self.rouge_scores = []\n",
        "        self.step_numbers = []\n",
        "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.run_id = run_id  # Store the run_id\n",
        "\n",
        "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
        "        if state.global_step % self.eval_steps == 0:\n",
        "            # Ensure we're in the correct MLflow run context\n",
        "            with mlflow.start_run(run_id=self.run_id, nested=True) as run:\n",
        "                model.eval()\n",
        "                sample_size = min(10, len(self.eval_dataset))\n",
        "                indices = np.random.choice(len(self.eval_dataset), sample_size, replace=False)\n",
        "\n",
        "                bleu_scores = []\n",
        "                rouge1_scores = []\n",
        "                rouge2_scores = []\n",
        "                rougeL_scores = []\n",
        "\n",
        "                for idx in indices:\n",
        "                    inputs = self.eval_dataset[idx]\n",
        "                    input_ids = inputs['input_ids'].unsqueeze(0).to(model.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model.generate(\n",
        "                            input_ids,\n",
        "                            max_new_tokens=128,  # Add this line to control the number of new tokens generated\n",
        "                            num_return_sequences=1,\n",
        "                            pad_token_id=self.tokenizer.pad_token_id\n",
        "                        )\n",
        "\n",
        "                    predicted_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                    target_text = self.tokenizer.decode(inputs['labels'], skip_special_tokens=True)\n",
        "\n",
        "                    # Calculate BLEU score\n",
        "                    smoothing = SmoothingFunction().method1\n",
        "                    bleu_score = sentence_bleu(\n",
        "                        [target_text.split()],\n",
        "                        predicted_text.split(),\n",
        "                        smoothing_function=smoothing\n",
        "                    )\n",
        "                    bleu_scores.append(bleu_score)\n",
        "\n",
        "                    # Calculate ROUGE scores\n",
        "                    rouge_scores = self.scorer.score(target_text, predicted_text)\n",
        "                    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
        "                    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
        "                    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "                # Get current loss\n",
        "                current_loss = state.log_history[-1].get('loss', 0) if state.log_history else 0\n",
        "\n",
        "                # Log metrics\n",
        "                metrics = {\n",
        "                    'step': state.global_step,\n",
        "                    'loss': current_loss,\n",
        "                    'bleu_score': np.mean(bleu_scores),\n",
        "                    'rouge1_score': np.mean(rouge1_scores),\n",
        "                    'rouge2_score': np.mean(rouge2_scores),\n",
        "                    'rougeL_score': np.mean(rougeL_scores)\n",
        "                }\n",
        "\n",
        "                # Log metrics to MLflow\n",
        "                mlflow.log_metrics(metrics, step=state.global_step)\n",
        "                print(metrics)\n",
        "\n",
        "                # Store metrics for plotting\n",
        "                self.losses.append(current_loss)\n",
        "                self.bleu_scores.append(np.mean(bleu_scores))\n",
        "                self.step_numbers.append(state.global_step)\n",
        "\n",
        "                # Create and log plots\n",
        "                if len(self.step_numbers) > 1:\n",
        "                    # Loss plot\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.plot(self.step_numbers, self.losses)\n",
        "                    plt.xlabel('Training Steps')\n",
        "                    plt.ylabel('Loss')\n",
        "                    plt.title('Training Loss Over Time')\n",
        "                    mlflow.log_figure(plt.gcf(), f\"loss_plot_step_{state.global_step}.png\")\n",
        "                    plt.close()\n",
        "\n",
        "                    # BLEU score plot\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.plot(self.step_numbers, self.bleu_scores)\n",
        "                    plt.xlabel('Training Steps')\n",
        "                    plt.ylabel('BLEU Score')\n",
        "                    plt.title('BLEU Score Over Time')\n",
        "                    mlflow.log_figure(plt.gcf(), f\"bleu_plot_step_{state.global_step}.png\")\n",
        "                    plt.close()\n",
        "\n",
        "                model.train()\n",
        "\n",
        "def train_model(model, tokenizer, dataset, training_args):\n",
        "    \"\"\"Train the model with MLflow tracking.\"\"\"\n",
        "    with mlflow.start_run(run_name=\"MODEL_TRAINING\") as run:\n",
        "        # Log training parameters\n",
        "        mlflow.log_params(training_args)\n",
        "\n",
        "        # Initialize custom metrics callback with the current run_id\n",
        "        metrics_callback = MetricsCallback(\n",
        "            tokenizer=tokenizer,\n",
        "            eval_dataset=dataset,\n",
        "            eval_steps=100,\n",
        "            run_id=run.info.run_id  # Pass the run_id to the callback\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=TrainingArguments(**training_args),\n",
        "            train_dataset=dataset,\n",
        "            data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "            callbacks=[metrics_callback]\n",
        "        )\n",
        "\n",
        "        # Train the model and log metrics\n",
        "        train_result = trainer.train()\n",
        "        mlflow.log_metrics(train_result.metrics)\n",
        "\n",
        "        # Save the model's state dictionary\n",
        "        model_path = \"model\"\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "        torch.save(trainer.model.state_dict(), os.path.join(model_path, \"model_state_dict.pth\"))\n",
        "\n",
        "        # Log the model directory as an artifact\n",
        "        mlflow.log_artifacts(model_path, artifact_path=\"model\")\n",
        "\n",
        "        # Register the model with MLflow\n",
        "        mlflow.register_model(\n",
        "            model_uri=f\"runs:/{run.info.run_id}/model\",\n",
        "            name=MODEL_NAME\n",
        "        )\n",
        "\n",
        "        return trainer, run.info.run_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtIJuCjmhDH3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "def setup_mlflow():\n",
        "    # Define the base directory for MLflow\n",
        "    base_dir = \"/content/mlflow\"\n",
        "\n",
        "    # Clean up any existing mlruns.db file that might be causing conflicts\n",
        "    if os.path.exists(\"/content/mlruns.db\"):\n",
        "        os.remove(\"/content/mlruns.db\")\n",
        "\n",
        "    # Create a fresh directory for MLflow\n",
        "    if os.path.exists(base_dir):\n",
        "        shutil.rmtree(base_dir)\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    # Set up MLflow to use the local directory\n",
        "    mlflow.set_tracking_uri(f\"file:{base_dir}\")\n",
        "\n",
        "    # Create the .trash directory if it doesn't exist\n",
        "    trash_dir = os.path.join(base_dir, \".trash\")\n",
        "    os.makedirs(trash_dir, exist_ok=True)\n",
        "\n",
        "    return MlflowClient()\n",
        "\n",
        "def main():\n",
        "    # Initialize MLflow with proper setup\n",
        "    client = setup_mlflow()\n",
        "\n",
        "    # Setup experiment with better error handling\n",
        "    experiment_name = \"documentation_generator\"\n",
        "    try:\n",
        "        print(f\"Attempting to create experiment: {experiment_name}\")\n",
        "        experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        print(f\"Successfully created experiment with ID: {experiment_id}\")\n",
        "    except mlflow.exceptions.MlflowException as e:\n",
        "        print(f\"Could not create experiment: {str(e)}\")\n",
        "        try:\n",
        "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "            if experiment is None:\n",
        "                raise Exception(f\"Experiment {experiment_name} not found\")\n",
        "            experiment_id = experiment.experiment_id\n",
        "            print(f\"Found existing experiment with ID: {experiment_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting experiment: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # Verify experiment is set\n",
        "    active_experiment = mlflow.get_experiment(experiment_id)\n",
        "    print(f\"Active experiment: {active_experiment.name} (ID: {active_experiment.experiment_id})\")\n",
        "    print(f\"Artifact location: {active_experiment.artifact_location}\")\n",
        "\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    # Training configuration\n",
        "    training_args = {\n",
        "        \"output_dir\": \"documentation-assistant\",\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"per_device_train_batch_size\": 4,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"warmup_ratio\": 0.05,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"fp16\": True,\n",
        "        \"logging_steps\": 100,\n",
        "        \"save_strategy\": \"epoch\",\n",
        "    }\n",
        "\n",
        "    # Execute ML pipeline with MLflow tracking\n",
        "    model, tokenizer, model_setup_run = setup_model_and_tokenizer(\"google/gemma-2b\")\n",
        "    dataset_info, data_prep_run = load_and_prepare_dataset(num_examples=600)\n",
        "    # Convert our dataset_info into a pytorch Dataset using this new function\n",
        "    dataset = prepare_training_data(dataset_info,tokenizer)\n",
        "    trainer, training_run = train_model(model, tokenizer, dataset, training_args)\n",
        "\n",
        "    # Wait for model registration to complete\n",
        "    import time\n",
        "    time.sleep(10)  # Wait for 10 seconds (Adjust if necessary)\n",
        "\n",
        "    # Get the latest model version\n",
        "    # Get or create the registered model\n",
        "    try:\n",
        "        registered_model = client.get_registered_model(name=MODEL_NAME)\n",
        "    except mlflow.exceptions.MlflowException:\n",
        "        # If the model doesn't exist, create it\n",
        "        registered_model = client.create_registered_model(name=MODEL_NAME)\n",
        "\n",
        "    # Fetch the latest versions\n",
        "    latest_versions = registered_model.latest_versions\n",
        "\n",
        "    # Transition latest model version to production only if latest_versions is not empty\n",
        "    if latest_versions:  # Check if latest_versions is not empty\n",
        "        client.transition_model_version_stage(\n",
        "            name=MODEL_NAME,\n",
        "            version=latest_versions[0].version,\n",
        "            stage=\"production\"\n",
        "        )\n",
        "    else:\n",
        "        print(f\"No registered model versions found for {MODEL_NAME}. Skipping transition to production.\")\n",
        "\n",
        "    # Print registered models\n",
        "    print(\"List of all registered models\")\n",
        "    print(\"=\" * 80)\n",
        "    # Use search_registered_models instead of list_registered_models\n",
        "    [print(rm) for rm in client.search_registered_models()] # Changed line\n",
        "\n",
        "    # Print specific model versions\n",
        "    print(f\"List of Model = {MODEL_NAME} and Versions\")\n",
        "    print(\"=\" * 80)\n",
        "    [print(mv) for mv in client.search_model_versions(f\"name='{MODEL_NAME}'\")]\n",
        "\n",
        "    return {\n",
        "        \"model_setup_run\": model_setup_run,\n",
        "        \"data_prep_run\": data_prep_run,\n",
        "        \"training_run\": training_run\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnDSXWwE9Clo"
      },
      "outputs": [],
      "source": [
        "!chmod 777 /content/mlruns.db  # Grants full permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVyf_6Fg_w6c"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken \"YOUR NGROK TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtHh_f3n-rRj"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Path to the MLflow model artifact\n",
        "artifact_uri = \"/content/model/model_state_dict.pth\"  # Direct path to the artifact\n",
        "\n",
        "# Download the artifact locally\n",
        "local_path = mlflow.artifacts.download_artifacts(artifact_uri=artifact_uri)\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the tokenizer and base model\n",
        "base_model_name = \"google/gemma-2b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
        "\n",
        "# Load the fine-tuned state dictionary onto the GPU\n",
        "state_dict = torch.load(local_path, map_location=device)\n",
        "\n",
        "# Update the base model with the fine-tuned weights\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Test the model with an example input\n",
        "test_input = \"generate a simple python documentation\"\n",
        "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)  # Move inputs to GPU\n",
        "\n",
        "# Generate predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Decode the output\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rby7iCRksPiS"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Download the model artifact\n",
        "local_path = mlflow.artifacts.download_artifacts(artifact_uri=model_uri)\n",
        "\n",
        "# Load the model state dictionary\n",
        "state_dict = torch.load(os.path.join(local_path, \"model_state_dict.pth\"))\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer, _ = setup_model_and_tokenizer(\"google/gemma-2b\")  # Assuming setup_model_and_tokenizer is defined\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rva6BzLV8YeI"
      },
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"/content/mlruns.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl67r3f3lgjp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Manually clear GPU memory\n",
        "torch.cuda.empty_cache()  # Clears the cache\n",
        "torch.cuda.reset_peak_memory_stats()  # Resets peak memory tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUKhQ54EGkv3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.memory_allocated(), torch.cuda.memory_reserved()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbQpG-s0GgyN"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
